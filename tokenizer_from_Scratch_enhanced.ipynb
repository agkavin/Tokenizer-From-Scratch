{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_chunks(file_path, chunk_size=1024*1024):  # Default chunk size is 1MB\n",
    "    with open(file_path, \"r\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "\n",
    "def process_file(file_path):\n",
    "    for chunk in read_in_chunks(file_path):\n",
    "        tokens = chunk.encode(\"utf-8\")  # Convert the chunk to raw bytes\n",
    "        tokens = list(map(int, tokens))  # Convert to a list of integers\n",
    "        # Here you can process the tokens as needed\n",
    "        # For example, you might aggregate results or perform analysis on this chunk\n",
    "\n",
    "        # For demonstration, let's just print the length of the processed chunk\n",
    "        print(\"Chunk length:\", len(tokens))\n",
    "    return tokens\n",
    "\n",
    "# Usage\n",
    "tokens = process_file(\"data/wikitext-103/wiki.train.tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_stats(ids):\n",
    "    pairs = zip(ids, ids[1:])  # Pythonic way to iterate consecutive elements\n",
    "    counts = Counter(pairs)    # Count occurrences using Counter\n",
    "    return counts\n",
    "\n",
    "def print_sorted_stats(stats):\n",
    "    sorted_stats = sorted(((v, k) for k, v in stats.items()), reverse=True)\n",
    "    for count, pair in sorted_stats:\n",
    "        print(f\"Pair: {pair}, Count: {count}\")\n",
    "\n",
    "# Example usage\n",
    "stats = get_stats(tokens)\n",
    "print_sorted_stats(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101),chr(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most repeated pair: (101, 32) with count: 13076\n"
     ]
    }
   ],
   "source": [
    "# Assuming stats is a dictionary of token pairs and their counts\n",
    "top_pair = max(stats, key=stats.get)  # Find the most repeated pair\n",
    "top_count = stats[top_pair]           # Get the count of the most repeated pair\n",
    "\n",
    "print(f\"Most repeated pair: {top_pair} with count: {top_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top pair: (101, 32)\n",
      "Sample tokens before merge: [121, 32, 104, 101, 97, 114, 100, 32, 116, 104, 105, 115, 32, 44, 32, 109, 97, 110, 121, 32]\n",
      "Sample tokens after merge: [121, 32, 104, 101, 97, 114, 100, 32, 116, 104, 105, 115, 32, 44, 32, 109, 97, 110, 121, 32]\n",
      "Length of tokens after merge: 428674\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# Debugging information\n",
    "print(\"Top pair:\", top_pair)\n",
    "print(\"Sample tokens before merge:\", tokens[:20])  # Print first 20 tokens for inspection\n",
    "\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(\"Sample tokens after merge:\", tokens2[:20])  # Print first 20 tokens for inspection\n",
    "print(\"Length of tokens after merge:\", len(tokens2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_stats(ids):\n",
    "    pairs = zip(ids, ids[1:])\n",
    "    counts = Counter(pairs)  # Use Counter for more efficient counting\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        if i < len(ids) - 1 and ids[i] == pair[0] and ids[i + 1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 1000  # Desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens)  # Copy so we don't destroy the original list\n",
    "\n",
    "merges = {}  # (int, int) -> int\n",
    "\n",
    "for i in range(num_merges):\n",
    "    stats = get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)\n",
    "    idx = 256 + i\n",
    "    print(f\"Merging pair {pair} into new token {idx}\")\n",
    "    ids = merge(ids, pair, idx)\n",
    "    merges[pair] = idx\n",
    "\n",
    "print(f\"Final vocabulary size: {len(set(ids))}\")  # Check final vocabulary size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 441750\n",
      "ids length: 165955\n",
      "compression ratio: 2.66X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "  return text\n",
    "\n",
    "#example for error replace\n",
    "print(decode([128]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104]\n",
      "[]\n",
      "[104, 593, 272, 438, 108, 100]\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # Convert the input string to a list of UTF-8 byte values\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    \n",
    "    # Perform token merging until no more pairs can be merged\n",
    "    while len(tokens) >= 2:\n",
    "        stats = get_stats(tokens)  # Get statistics of consecutive pairs\n",
    "        # Select the pair with the minimum frequency that can be merged\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break  # Stop if no more pairs can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)  # Merge the selected pair\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "print(encode(\"h\"))         # Test with a single character\n",
    "print(encode(\"\"))          # Test with an empty string\n",
    "print(encode(\"hello world\"))  # Test with a regular string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "hello world\n"
     ]
    }
   ],
   "source": [
    "# Encode and then decode a sample string\n",
    "sample_string = \"hello world\"\n",
    "encoded = encode(sample_string)\n",
    "decoded = decode(encoded)\n",
    "\n",
    "# Print to verify correctness\n",
    "print(decoded == sample_string)  # Should print True if encoding and decoding work correctly\n",
    "print(decoded)  # Should print \"hello world\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def read_in_chunks(file_path, chunk_size=1024*1024):  # 1MB default chunk size\n",
    "    with open(file_path, \"r\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            yield chunk\n",
    "\n",
    "def process_file_chunks(file_path):\n",
    "    encoded_chunks = []\n",
    "    decoded_chunks = []\n",
    "\n",
    "    # Process chunks for encoding\n",
    "    for chunk in read_in_chunks(file_path):\n",
    "        encoded_chunks.append(encode(chunk))\n",
    "\n",
    "    # Flatten the list of lists into a single list\n",
    "    encoded_data = [token for chunk in encoded_chunks for token in chunk]\n",
    "\n",
    "    # Process chunks for decoding\n",
    "    decoded_chunks.append(decode(encoded_data))\n",
    "\n",
    "    # Join all decoded chunks to form the final text\n",
    "    final_decoded_text = ''.join(decoded_chunks)\n",
    "    return final_decoded_text\n",
    "\n",
    "# File path to the validation data\n",
    "file_path = \"data/wikitext-103/wiki.valid.tokens\"\n",
    "\n",
    "# Process the file in chunks\n",
    "final_decoded_text = process_file_chunks(file_path)\n",
    "\n",
    "# Verify correctness\n",
    "with open(file_path, \"r\") as f:\n",
    "    original_text = f.read()\n",
    "\n",
    "print(final_decoded_text == original_text)  # Should print True if encoding and decoding work correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with indices have been exported to tokens_with_indices.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Assuming 'vocab' is your final dictionary mapping indices to byte sequences\n",
    "# We want to reverse it to map byte sequences (as strings) to indices\n",
    "\n",
    "# Convert byte sequences to strings for JSON serialization\n",
    "export_vocab = {idx: value.decode(\"utf-8\", errors=\"replace\") for idx, value in vocab.items()}\n",
    "\n",
    "# Define the file path for the JSON file\n",
    "json_file_path = \"tokens_with_indices.json\"\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "with open(json_file_path, \"w\") as json_file:\n",
    "    json.dump(export_vocab, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Tokens with indices have been exported to {json_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded token-to-index mapping from JSON.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "json_file_path = \"tokens_with_indices.json\"\n",
    "\n",
    "with open(json_file_path, \"r\") as json_file:\n",
    "    token_to_index = json.load(json_file)\n",
    "\n",
    "# Reverse the dictionary for decoding: index -> token\n",
    "index_to_token = {int(idx): token for idx, token in token_to_index.items()}\n",
    "\n",
    "print(\"Loaded token-to-index mapping from JSON.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100]\n",
      "Decoded: hello world\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # Convert text to a list of byte values (tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    \n",
    "    encoded_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        for length in range(2, 0, -1):  # Try to find the longest match first\n",
    "            if i + length <= len(tokens):\n",
    "                sub_token = tuple(tokens[i:i+length])\n",
    "                if sub_token in token_to_index:\n",
    "                    encoded_tokens.append(token_to_index[sub_token])\n",
    "                    i += length\n",
    "                    break\n",
    "        else:  # If no match is found, just use the single byte value\n",
    "            encoded_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    \n",
    "    return encoded_tokens\n",
    "\n",
    "def decode(encoded_tokens):\n",
    "    # Convert list of indices back to text\n",
    "    decoded_bytes = b\"\".join(index_to_token[idx].encode(\"utf-8\") for idx in encoded_tokens)\n",
    "    decoded_text = decoded_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    return decoded_text\n",
    "\n",
    "# Test encoding and decoding\n",
    "sample_text = \"hello world\"\n",
    "encoded = encode(sample_text)\n",
    "print(f\"Encoded: {encoded}\")\n",
    "\n",
    "decoded = decode(encoded)\n",
    "print(f\"Decoded: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Load the existing tokenizer from the JSON file\n",
    "hf_tokenizer = Tokenizer.from_file(\"data/tokenizer-wiki.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hugging Face Encoded: [1299, 3225, 5595, 13762, 20804, 1401, 1257, 29578, 5243, 18]\n",
      "Hugging Face Decoded: The quick brown fox jumps over the lazy dog .\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "test_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Encode the text using the Hugging Face tokenizer\n",
    "hf_encoded = hf_tokenizer.encode(test_text)\n",
    "print(f\"Hugging Face Encoded: {hf_encoded.ids}\")\n",
    "\n",
    "# Decode the tokens back to text\n",
    "hf_decoded = hf_tokenizer.decode(hf_encoded.ids)\n",
    "print(f\"Hugging Face Decoded: {hf_decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test text\n",
    "\n",
    "test_text = \"\"\" Shortages of aircraft and serviceability problems greatly retarded pilot training and the ships only had a total of 17 D4Ys and 18 <unk> on hand on 1 October ; of these , only 6 and 16 were operational , respectively . The Japanese plan for the defence of the Philippines envisioned that the surviving carriers would be used to lure the American carrier forces away from the invasion area to a position where the carriers could be attacked by land @-@ based aircraft and the transports by the rest of the IJN . The other carrier air groups were not in much better shape and the Japanese decided to retain the aircraft ashore for use against the American carriers . The Fourth Carrier Division was assigned to the Northern Force under the command of Vice Admiral Jisaburō Ozawa and the sisters sailed from Yashima on 20 October . On the morning of 24 October , the bulk of the few aircraft aboard were launched to attack the American carriers as a distraction . They inflicted no damage and caused the Americans to search in the direction from which they had attacked . The Americans finally spotted the Japanese carriers at 16 : 40 , some 200 miles ( 320 km ) east of Cape Engaño , the northeastern tip of Luzon . The American carriers were spread out and it was very late in the day to launch an airstrike , so Admiral William Halsey , commander of the Third Fleet decided to mass his carriers in a position to attack the following morning . Ozawa reversed course during the night , correctly believing that the Americans would follow him north . \n",
    " Although they had lost contact during the night , the Americans did find the Japanese carriers at 07 : 35 . They had already launched an airstrike of 180 aircraft that was orbiting 50 miles ( 80 km ) ahead of the American carriers while waiting for the Japanese ships to be located . This was just the first of a total of five airstrikes that the Americans launched that day . The sisters were not heavily engaged by the early airstrikes which are focusing on the group 's aircraft carriers . Ise claimed to have shot down five attacking dive bombers from the second wave and one small bomb detonated on Turret No. 2 . Hyūga was lightly damaged by near misses that rupture some hull plating in her bulge and pepper her superstructure with splinters . She took on a 5 @-@ degree list that was quickly corrected before she was ordered to tow the crippled carrier Chiyoda to safety . Her attempt was unsuccessful and Chiyoda had to be abandoned to her fate . \n",
    " Ise was attacked by 80 @-@ odd aircraft from the fourth wave , but they failed to inflict any serious damage . She dodged 11 torpedoes and was only hit by a bomb once , on the bulge outboard of the port catapult . Some 34 other bombs near missed her , spraying her with splinters and ruptured some hull plates that contaminated some fuel oil and caused leaks in her port boiler rooms . While an exact total of her casualties is not available , it has been estimated that 5 men were killed and some 111 – 121 crewmen were wounded during this attack . Hyūga was unsuccessfully attacked by an American submarine at 18 : 43 . Around 19 : 00 Ozawa learned about a force of destroyers and cruisers that drove off the Japanese destroyers rescuing survivors from some of the carriers lost earlier in the day and sank Chiyoda . He ordered the Fourth Carrier Division to reverse course and engage the Americans , but the battleships were unable to find them , and Ozawa ordered them to reverse course and head for Amami Ōshima . When they arrived on 27 October , Ozawa transferred to Hyūga and hoisted his flag aboard her . While en route for Kure , the division was unsuccessfully attacked by another submarine . \n",
    " In early November the catapults were removed from both ships , and they loaded troops and munitions later that month . While en route they were diverted to the Spratly Islands upon reports of heavy air raids at Manila . After off @-@ loading their cargo , they sailed for Lingga Island , near Singapore , on 20 November . They transferred to Cam Ranh Bay , French Indochina and Hyūga became flagship of the 5th Fleet there on 14 December . The division sailed for Singapore on 30 December and Vice Admiral Kiyohide Shima transferred his flag to the light cruiser Ōyodo on arrival there the following day . The division continued onwards to Lingga . Its planned return to Japan was delayed by attacks by the American Third Fleet on targets in Indochina and southern China that sank two oil tankers that were intended to refuel the division . \n",
    " The IJN then decided to use the sisters and their escorts to bring a load of petrol , rubber , tin and other strategic minerals back to Japan after the American carriers departed the South China Sea ( Operation Kita ) . They loaded their cargoes beginning on 6 February at Singapore and departed four days later . Also carrying some 1 @,@ 150 oilfield workers , they were escorted by Ōyodo and three destroyers . <unk> Japanese radio signals revealed the Japanese plan to the Allies , and 15 submarines were positioned along their anticipated route in an attempt to intercept and sink the ships . An additional 11 were moved into position while the group was en route , but only three were ultimately able to attack . None of them were successful before the Japanese reached Kure on 20 February . The Fourth Carrier Division was disbanded on 1 March and the sisters were reduced to 1st rank reserve ships . On 19 March Kure was attacked by aircraft from Task Force 58 and Hyūga was hit three times by bombs that killed 37 men and wounded 52 . Her gunners claimed to have shot down one American dive bomber during the attack . Ise was hit twice during the attack , but her casualties , if any , are unknown . \n",
    " The ships were turned into floating AA batteries over the next several months although it availed them little when they were attacked again by American carrier aircraft in July . On the 24th Ise was struck by five bombs and near missed multiple times ; all told she lost 50 crewmen killed and many others wounded . The bombs started numerous leaks and Ise began to settle by the bow , although she was returned to an even keel after three @-@ days pumping . Hyūga was a primary focus of the attack and she received 10 direct hits and up to 30 near misses . She was badly damaged with some 200 @-@ odd crewmen killed and 600 wounded during the attack . She slowly foundered over the next two days and was not attacked when the Americans returned four days later . This time it was Ise 's turn and she was struck 11 or more times with many near misses that put her on the bottom in shallow water with a 15 degree list . The sisters were struck off the Navy List in November and their wrecks were scrapped after the war . \n",
    " \"\"\"\n",
    "\n",
    "# Measure time for your custom tokenizer\n",
    "start = time.time()\n",
    "custom_encoded = encode(test_text)\n",
    "custom_decoded = decode(custom_encoded)\n",
    "custom_time = time.time() - start\n",
    "\n",
    "# Measure time for the Hugging Face tokenizer\n",
    "start = time.time()\n",
    "hf_encoded = hf_tokenizer.encode(test_text)\n",
    "hf_decoded = hf_tokenizer.decode(hf_encoded.ids)\n",
    "hf_time = time.time() - start\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.011239 sec\n",
      "\n",
      "Time: 0.004477 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For Custom Tokenizer\n",
    "custom_tokens = [vocab[idx].decode('utf-8', errors='replace') for idx in custom_encoded]\n",
    "#print(f\"Custom Tokenizer - Encoded: {custom_encoded}\\nDecoded: {custom_decoded}\\nTokens: {custom_tokens}\\n\n",
    "print(f\"Time: {custom_time:.6f} sec\\n\")\n",
    "\n",
    "# For Hugging Face Tokenizer\n",
    "hf_tokens = [hf_tokenizer.id_to_token(idx) for idx in hf_encoded.ids]\n",
    "#print(f\"Hugging Face Tokenizer - Encoded: {hf_encoded.ids}\\nDecoded: {hf_decoded}\\nTokens: {hf_tokens}\\nTime: {hf_time:.6f} sec\\n\")\n",
    "print(f\"Time: {hf_time:.6f} sec\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
